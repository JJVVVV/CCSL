{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "dataset_name = 'MRPC'\n",
    "dataset = load_dataset(\"glue\", dataset_name)\n",
    "# you can use any of the following config names as a second argument:\n",
    "\"ax\", \"cola\", \"mnli\", \"mnli_matched\", \n",
    "\"mnli_mismatched\", \"mrpc\", \"qnli\", \"qqp\", \n",
    "\"rte\", \"sst2\", \"stsb\", \"wnli\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jsonlines\n",
    "import os\n",
    "from tqdm.auto import trange\n",
    "dataset_name = dataset_name.upper()\n",
    "splits = dataset.keys()\n",
    "# split = 'validation'\n",
    "for split in splits:\n",
    "    output_path = f\"data/{dataset_name}/{split}\"\n",
    "    os.makedirs(output_path, exist_ok=True)\n",
    "    with jsonlines.open(os.path.join(output_path, 'all.jsonl'), 'w') as jlWriter:\n",
    "        objs = []\n",
    "        keys = list(dataset[split].features.keys())\n",
    "        data_dict = {key:dataset[split][key] for key in keys}\n",
    "        for i in trange(dataset[split].num_rows):\n",
    "            objs.append({key:data_dict[key][i] for key in keys})\n",
    "        jlWriter.write_all(objs)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://huggingface.co/Qwen/Qwen-14B-Chat/resolve/main/model-00001-of-00015.safetensors\n",
      "https://huggingface.co/Qwen/Qwen-14B-Chat/resolve/main/model-00002-of-00015.safetensors\n",
      "https://huggingface.co/Qwen/Qwen-14B-Chat/resolve/main/model-00003-of-00015.safetensors\n",
      "https://huggingface.co/Qwen/Qwen-14B-Chat/resolve/main/model-00004-of-00015.safetensors\n",
      "https://huggingface.co/Qwen/Qwen-14B-Chat/resolve/main/model-00005-of-00015.safetensors\n",
      "https://huggingface.co/Qwen/Qwen-14B-Chat/resolve/main/model-00006-of-00015.safetensors\n",
      "https://huggingface.co/Qwen/Qwen-14B-Chat/resolve/main/model-00007-of-00015.safetensors\n",
      "https://huggingface.co/Qwen/Qwen-14B-Chat/resolve/main/model-00008-of-00015.safetensors\n",
      "https://huggingface.co/Qwen/Qwen-14B-Chat/resolve/main/model-00009-of-00015.safetensors\n",
      "https://huggingface.co/Qwen/Qwen-14B-Chat/resolve/main/model-00010-of-00015.safetensors\n",
      "https://huggingface.co/Qwen/Qwen-14B-Chat/resolve/main/model-00011-of-00015.safetensors\n",
      "https://huggingface.co/Qwen/Qwen-14B-Chat/resolve/main/model-00012-of-00015.safetensors\n",
      "https://huggingface.co/Qwen/Qwen-14B-Chat/resolve/main/model-00013-of-00015.safetensors\n",
      "https://huggingface.co/Qwen/Qwen-14B-Chat/resolve/main/model-00014-of-00015.safetensors\n",
      "https://huggingface.co/Qwen/Qwen-14B-Chat/resolve/main/model-00015-of-00015.safetensors\n"
     ]
    }
   ],
   "source": [
    "for x in range(1, 16):\n",
    "    url = f\"https://huggingface.co/Qwen/Qwen-14B-Chat/resolve/main/model-000{x:02d}-of-00015.safetensors\"\n",
    "    print(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import hf_hub_download\n",
    "\n",
    "model_name = \"Qwen/Qwen-14B-Chat\"\n",
    "local_dir = f\"../../pretrained/{model_name}\"\n",
    "hf_hub_download(\n",
    "    repo_id=model_name,\n",
    "    filename=\"model-00008-of-00015.safetensors\",\n",
    "    cache_dir=\"/data/jjwang/.cache/hf_cache\",\n",
    "    local_dir=local_dir,\n",
    "    local_dir_use_symlinks=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "model_name=\"Qwen/Qwen-14B-Chat\"\n",
    "local_dir=\"../../pretrained/${model_name}\"\n",
    "nohup huggingface-cli download $model_name --exclude \".safetensors\" --local-dir $local_dir --local-dir-use-symlinks False --cache-dir /data/jjwang/.cache/hf_cache > ./downloading.log 2>&1 &"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "model_name=\"bert-base-chinese\"\n",
    "local_dir=\"../../pretrained/${model_name}\"\n",
    "huggingface-cli download $model_name --exclude \"*.safetensors\" --exclude \"*.h5\" --exclude \"*.ot\" --exclude \"*.safetensors\" --exclude \"*.msgpack\" --local-dir $local_dir --local-dir-use-symlinks False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocess and Concat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 40430 entries, 0 to 40429\n",
      "Data columns (total 5 columns):\n",
      " #   Column     Non-Null Count  Dtype \n",
      "---  ------     --------------  ----- \n",
      " 0   question1  40430 non-null  object\n",
      " 1   question2  40430 non-null  object\n",
      " 2   label      40430 non-null  int64 \n",
      " 3   rephrase1  40430 non-null  object\n",
      " 4   rephrase2  40430 non-null  object\n",
      "dtypes: int64(1), object(4)\n",
      "memory usage: 1.5+ MB\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "\n",
    "dataset = \"QQP\"\n",
    "split = \"val\"\n",
    "dataset2lang = {\"LCQMC\": \"zh\", \"QQP\": \"en\", \"BQ\": \"zh\", \"MRPC\":\"en\", \"QNLI\":'en'}\n",
    "lang = dataset2lang[dataset]\n",
    "# file_dir = Path(f'generation/results/qwen-14b-chat/QQP/{split}/Rephrase this sentence./')\n",
    "file_dir = Path(f'generation/results/qwen-14b-chat/{dataset}/{split}/rephrase')\n",
    "\n",
    "df = pd.DataFrame()\n",
    "\n",
    "for path in sorted(file_dir.glob(\"*.jsonl\"), key=lambda path: int(path.name.split('-')[0])):\n",
    "    part = pd.read_json(path, lines=True)\n",
    "    df = pd.concat((df, part), axis=0)\n",
    "df.reset_index(drop=True, inplace=True)\n",
    "df.info()\n",
    "# df.to_json('data/QQP/train/with_rephrase.jsonl', force_ascii=False, lines=True, orient='records')\n",
    "# print(df['reason'].str.len().max())\n",
    "# print(df['reason'].str.len().median())\n",
    "# print(df['reason'].str.len().mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "94\n",
      "35\n",
      "Does Mike Wazowski blink or wink?\n",
      "------------\n",
      "Is Mike Wazowski known for blinking or winking?\n",
      "===============================================\n",
      "Does Mike Wazowski blinks or wink?\n",
      "------------\n",
      "Does Mike Wazowski blink or wink? (The same meaning as the original sentence) \n",
      "\n",
      "OR\n",
      "\n",
      "Is it Mike Wazowski blinking or winking? (Slightly different phrasing but with the same meaning)\n",
      "######################################################################\n",
      "What the purpose of life on earth?\n",
      "------------\n",
      "The intended meaning of the original sentence remains the same after rephrasing:\n",
      "\n",
      "\"What is the reason for our existence on Earth?\"\n",
      "===============================================\n",
      "What is ultimate purpose of life?\n",
      "------------\n",
      "Can you please provide me with the original sentence so I can rephrase it while maintaining its intended meaning?\n",
      "######################################################################\n",
      "My cat had miscarriage 2 days earlier and she is stil bleeding unable to sit properly, seems to be in pain?\n",
      "------------\n",
      "The original sentence: \"My cat had a miscarriage 2 days earlier and she is still bleeding, unable to sit properly, and seems to be in pain?\"\n",
      "\n",
      "Rephrased sentence: \"My cat experienced a miscarriage two days ago and is currently bleeding, struggling to sit comfortably, and appears to be in discomfort.\"\n",
      "===============================================\n",
      "Amazon great Indian sale 2016?\n",
      "------------\n",
      "Did Amazon have a Great Indian Sale in 2016?\n",
      "######################################################################\n",
      "Why set max telecast movie Suryavanshan on every week?\n",
      "------------\n",
      "\"Why air the same maximum telecasted movie 'Suryavanshan' every week?'\"\n",
      "(Note: The original sentence was already clear and concise, so I simply rephrased it while maintaining its intended meaning.)\n",
      "===============================================\n",
      "Why does Set Max repeatedly telecast Sooryavansham movie in India?\n",
      "------------\n",
      "What is the reason behind Set Max's repeated broadcast of the Sooryavansham movie in India?\n",
      "######################################################################\n",
      "Vegan sustitute for sour cream?\n",
      "------------\n",
      "Is there a vegan alternative for sour cream?\n",
      "===============================================\n",
      "Russia has decided to support Terror group called Taliban in Afghanistan where it is fighting US troops. Is Russia 's act justified?\n",
      "------------\n",
      "The original sentence states that Russia has chosen to provide support to the Taliban terrorist organization in Afghanistan, who are currently engaged in conflict with US forces. The question posed is whether or not this action by Russia can be considered justifiable.\n",
      "\n",
      "Rephrased sentence: Russia's decision to back the Taliban in their fight against US troops in Afghanistan raises the question of whether such an action can be deemed justifiable.\n",
      "######################################################################\n",
      "%3c%2fscript%3e%3cscript%3ealert(1) %3c%2fscript%3 %3c%2fscript%3e%3cscript%3ealert(1) %3c%2fscript%3 %3c%2fscript%3e%3cscript%3ealert(1) %3c%2fscript%3?\n",
      "------------\n",
      "The rewritten sentence without changing its meaning is:\n",
      "\n",
      "```php-template\n",
      "</script><script>alert(1)</script> </script><script>alert(1)</script> </script><script>alert(1)</script>\n",
      "```\n",
      "===============================================\n",
      "What is the mechanism of fast charging in the new smartphone batteries?\n",
      "------------\n",
      "How does the new smartphone battery achieve fast charging?\n",
      "######################################################################\n",
      "\"What do you think about the song\"\"Lean On\"\"?\"\n",
      "------------\n",
      "Can you share your opinion on the song \"Lean On\"? Please ensure that the intended meaning of the original sentence is preserved in your response.\n",
      "===============================================\n",
      "What do you think about this song? :-) x\n",
      "------------\n",
      "Can you share your opinion on this song? I'm curious to know what you think. :-)\n",
      "(Note: The emoticon and \"x\" at the end are not necessary for conveying the intended meaning and have been left out in the rephrased sentence.)\n",
      "######################################################################\n",
      "What is the best story you can write in only one line in malayalam?\n",
      "------------\n",
      "\"Malayalamdeyum ennum onnellathin katha pattum?\" (Can you write the happiest story in Malayalam in just one line?)\n",
      "===============================================\n",
      "What is the best story you can write in only one line in hindi?\n",
      "------------\n",
      "\"क्या तुम निर्दिष्ट संग्रहालय में ही खोजने के लिए आते हो?\" (Kya tum nirvishtha sangrahalyam mein hi koshne ke liye aate ho?) \n",
      "\n",
      "Note: This is a rephrased version of the original sentence in Hindi, but it does not convey the same meaning as the original English sentence. It translates to \"Do you come to the specific museum for research only?\" which is not related to the topic of writing a story in one line.\n",
      "######################################################################\n",
      "What is purpose of life?\n",
      "------------\n",
      "Can you please provide me with the original sentence?\n",
      "===============================================\n",
      "What the purpose of life on earth?\n",
      "------------\n",
      "The intended meaning of the original sentence remains the same after rephrasing:\n",
      "\n",
      "\"What is the reason for our existence on Earth?\"\n",
      "######################################################################\n",
      "\"Which is correct, \"\"I missed you\"\" or \"\"I've missed you\"\"?\"\n",
      "------------\n",
      "The question asks whether \"I missed you\" or \"I've missed you\" is grammatically correct and conveys the same meaning.\n",
      "===============================================\n",
      "Mailing's cat ___1) has been 2) had been 3) will be 4) was missing for three days. She is extremely sad and worried. Which one of the four answers is the correct one?\n",
      "------------\n",
      "Which of the following options correctly describes Mailing's cat being missing for three days? \n",
      "A) The cat has been missing for three days.\n",
      "B) The cat had been missing for three days.\n",
      "C) The cat will be missing for three days.\n",
      "D) The cat was missing for three days. \n",
      "\n",
      "Please select the option that accurately conveys the same meaning as the original sentence without changing its intent.\n",
      "######################################################################\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "from load_data_fns import key_map\n",
    "# from toolkit.nlp import word_count\n",
    "# long_mask = ((df['rephrase1'].apply(word_count)>thr) | (df['rephrase2'].apply(word_count)>thr))\n",
    "\n",
    "df['rephrase1'] = df['rephrase1'].str.strip()\n",
    "df['rephrase2'] = df['rephrase2'].str.strip()\n",
    "long_mask = ((df['rephrase1'].str.contains('\\n')) | (df['rephrase2'].str.contains('\\n')))\n",
    "print(long_mask.sum())\n",
    "\n",
    "if lang=='en':\n",
    "    def postprogess(s: str):\n",
    "        if '\\n' in s:\n",
    "            splited = s.split('\\n')\n",
    "            if 'rewritten as follows' in splited[0] or 'rephrased as follows' in splited[0] or 'rephrased sentence' in splited[0] or \"rephrased version\" in splited[0]:\n",
    "                s =  splited[-1].strip()\n",
    "                # print(s)\n",
    "            # elif (m:=re.search(r'(^Rephrased: )(.*)', splited[-1])):\n",
    "            #     print(s)\n",
    "            #     s =  m.group(2)\n",
    "            #     print(s)\n",
    "            # elif (m:=re.search(r'(^改写后\\：|改写为\\：)(.*)', splited[-1])):\n",
    "            #     print(m.group(2))\n",
    "            #     return m.group(2)\n",
    "        return s\n",
    "    df.loc[long_mask, 'rephrase1'] = df[long_mask]['rephrase1'].apply(postprogess)\n",
    "    df.loc[long_mask, 'rephrase2'] = df[long_mask]['rephrase2'].apply(postprogess)\n",
    "\n",
    "    long_mask = ((df['rephrase1'].str.contains('\\n')) | (df['rephrase2'].str.contains('\\n')))\n",
    "    print(long_mask.sum())\n",
    "\n",
    "masked_df = df[long_mask]\n",
    "df = df[~long_mask]\n",
    "\n",
    "for idx, row in masked_df[:10].iterrows():\n",
    "    print(row[key_map[dataset][0]], row['rephrase1'], sep='\\n------------\\n')\n",
    "    print(\"===============================================\")\n",
    "    print(row[key_map[dataset][1]], row['rephrase2'], sep='\\n------------\\n')\n",
    "    print(\"######################################################################\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pattern = r\"[\\w ']* original (sentence|text|meaning) .*\"\n",
    "# red_mask = df['rephrase1'].str.contains(pattern, regex=True) | df['rephrase2'].str.contains(pattern, regex=True)\n",
    "# for idx, row in df[red_mask].iterrows():\n",
    "#     print(row[key_map[dataset][0]], row['rephrase1'], sep='\\n------------\\n')\n",
    "#     print(\"===============================================\")\n",
    "#     print(row[key_map[dataset][1]], row['rephrase2'], sep='\\n------------\\n')\n",
    "#     print(\"######################################################################\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if lang=='en':\n",
    "#     def postprogess(s: str):\n",
    "#         pattern = r\"[\\w ']* original (sentence|text|meaning) .*\"\n",
    "#         # if re.search(pattern, s, flags=re.IGNORECASE):\n",
    "#             # print(s)\n",
    "#         s = re.sub(pattern, '', s, flags=re.IGNORECASE)\n",
    "#             # print(s)\n",
    "#         return s\n",
    "#     df['rephrase1'] = df['rephrase1'].apply(postprogess)\n",
    "#     df['rephrase2'] = df['rephrase2'].apply(postprogess)\n",
    "# for idx, row in df[red_mask].iterrows():\n",
    "#     print(row[key_map[dataset][0]], row['rephrase1'], sep='\\n------------\\n')\n",
    "#     print(\"===============================================\")\n",
    "#     print(row[key_map[dataset][1]], row['rephrase2'], sep='\\n------------\\n')\n",
    "#     print(\"######################################################################\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "155\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "# def postprogess(s: str):\n",
    "#     if re.search(r\"[\\w ']* original (sentence|text|meaning) .*\", s, flags=re.IGNORECASE):\n",
    "#         print('===============')\n",
    "#         print(s)\n",
    "#         print('----------')\n",
    "#         s = re.sub(r\"[\\w ']* original (sentence|text|meaning) .*\", '', s, flags=re.IGNORECASE)\n",
    "#         print(s)\n",
    "#         print('===============')\n",
    "#     return s\n",
    "if lang=='en':\n",
    "    def postprogess(s: str):\n",
    "        pattern = r\"[\\w ']* original (sentence|text|meaning) .*\"\n",
    "        # if re.search(pattern, s, flags=re.IGNORECASE):\n",
    "            # print(s)\n",
    "        s = re.sub(pattern, '', s, flags=re.IGNORECASE)\n",
    "            # print(s)\n",
    "        return s\n",
    "    df['rephrase1'] = df['rephrase1'].apply(postprogess)\n",
    "    df['rephrase2'] = df['rephrase2'].apply(postprogess)\n",
    "\n",
    "    blank_mask = ((df['rephrase1']=='') | (df['rephrase2']==''))\n",
    "    masked_df = df[blank_mask]\n",
    "    df = df[~blank_mask]\n",
    "    print(blank_mask.sum())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "785\n",
      "Why did the US invade Iraq in 2003?\n",
      "------------\n",
      "What was the reason for the United States' invasion of Iraq in 2003?\n",
      "===============================================\n",
      "What led to the US invading Iraq in 2003?\n",
      "------------\n",
      "Can you provide a rephrased version of the sentence that conveys the same meaning as the original?\n",
      "######################################################################\n",
      "What is the issue in Baluchistan?\n",
      "------------\n",
      "Can you please clarify what specifically you would like me to rephrase?\n",
      "===============================================\n",
      "What exactly is the issue of Baluchistan?\n",
      "------------\n",
      "Can you please clarify the specific problem in Baluchistan?\n",
      "######################################################################\n",
      "How did you make it into Stanford? What was your high school life like before Stanford?\n",
      "------------\n",
      "Can you tell me about your journey to Stanford and what your high school experience was like prior to attending? Please provide details on how you managed to gain admission to this prestigious university.\n",
      "===============================================\n",
      "\"\"\"keep looking, don't settle\"\" its about what you really want to do in your life- by Steve jobs in his famous Stanford speech. Tell me who ever found it, how did you find it?\"\n",
      "------------\n",
      "The rephrased sentence is: \"Steve Jobs emphasized in his iconic Stanford speech that one should persistently pursue their true passion and not compromise, as stated in the phrase 'keep looking, don't settle'.\"\n",
      "######################################################################\n",
      "What was that best movie you watched and why?\n",
      "------------\n",
      "Can you please tell me which movie you consider to be the best and what makes it so?\n",
      "===============================================\n",
      "What is the best movie you have ever seen?\n",
      "------------\n",
      "Can you please provide me with a rephrased version of the sentence \"What is the best movie you have ever seen?\" that conveys the same meaning?\n",
      "######################################################################\n",
      "What are the aspects of cell theory?\n",
      "------------\n",
      "Can you provide a rephrased version of the sentence \"What are the aspects of cell theory?\" that retains the original meaning?\n",
      "===============================================\n",
      "What are the three components of the cell theory?\n",
      "------------\n",
      "Can you please provide me with the original sentence? I need it to understand what needs to be rephrased while keeping the same meaning.\n",
      "######################################################################\n",
      "What is the best love making experience?\n",
      "------------\n",
      "Can you provide a rephrased version of the sentence \"What is the best love making experience?\" that conveys the same meaning?\n",
      "===============================================\n",
      "What was your best lovemaking experience?\n",
      "------------\n",
      "Can you describe your most memorable sexual encounter?\n",
      "######################################################################\n",
      "What's some of the best advice you've ever received?\n",
      "------------\n",
      "Can you share some of the most valuable pieces of advice that you have ever been given?\n",
      "===============================================\n",
      "What is the best advice on selling you ever received?\n",
      "------------\n",
      "Can you provide a rephrased version of the sentence \"What is the best advice on selling you ever received?\" that conveys the same meaning?\n",
      "######################################################################\n",
      "Will TCS cancel offer for campus placed students due to loss of US BFSI segments?\n",
      "------------\n",
      "Is it possible that TCS may terminate the job offers made to campus recruits as a result of losing their US BFSI business segments?\n",
      "===============================================\n",
      "What will be the expected date of joining for TCS placed students through off campus 2016 recruitment held in March?\n",
      "------------\n",
      "Can you please clarify if the rephrased sentence should convey the same meaning as the original sentence?\n",
      "######################################################################\n",
      "What is the conclusion of the movie Before we go?\n",
      "------------\n",
      "Can you provide a rephrased version of the sentence that conveys the same meaning as the original: \"What is the ending of the film Before we go?\"\n",
      "===============================================\n",
      "What is the explanation for the ending of the movie Before We Go?\n",
      "------------\n",
      "Can you provide an explanation for the conclusion of the film Before We Go?\n",
      "######################################################################\n",
      "Which has been the biggest political scam in India till date?\n",
      "------------\n",
      "What is the most significant political fraud that has taken place in India up until now?\n",
      "===============================================\n",
      "What is the biggest scam in India?\n",
      "------------\n",
      "Can you provide me with a rephrased version of the sentence \"What is the biggest scam in India?\" that retains its original meaning?\n",
      "######################################################################\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import re\n",
    "pt = re.compile(r'rephrase|改写')\n",
    "fail_mask = ((df['rephrase1'].str.contains(pt, regex=True)) | (df['rephrase2'].str.contains(pt, regex=True)))\n",
    "print(fail_mask.sum())\n",
    "masked_df = df[fail_mask]\n",
    "df = df[~fail_mask]\n",
    "\n",
    "for idx, row in masked_df[:10].iterrows():\n",
    "    print(row[key_map[dataset][0]], row['rephrase1'], sep='\\n------------\\n')\n",
    "    print(\"===============================================\")\n",
    "    print(row[key_map[dataset][1]], row['rephrase2'], sep='\\n------------\\n')\n",
    "    print(\"######################################################################\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from toolkit.nlp import punctuation_convert\n",
    "import re\n",
    "from load_data_fns import key_map\n",
    "pattern = re.compile(r'^\"|\"$|^“|”$|^”|“$')\n",
    "\n",
    "# key_map = {\n",
    "#     \"LCQMC\": (\"question1\", \"question2\"),\n",
    "#     \"BQ\": (\"question1\", \"question2\"),\n",
    "#     \"QQP\": (\"question1\", \"question2\"),\n",
    "#     \"MRPC\": (\"sentence1\", \"sentence2\"),\n",
    "# }\n",
    "df['rephrase1'] = df['rephrase1'].str.strip()\n",
    "df['rephrase2'] = df['rephrase2'].str.strip()\n",
    "df[key_map[dataset][0]] = df[key_map[dataset][0]].str.replace(pattern, '', regex=True)\n",
    "df[key_map[dataset][1]] = df[key_map[dataset][1]].str.replace(pattern, '', regex=True)\n",
    "df['rephrase1'] = df['rephrase1'].apply(lambda x: punctuation_convert(x, lang).strip()).str.replace(pattern, '', regex=True)\n",
    "df['rephrase2'] = df['rephrase2'].apply(lambda x: punctuation_convert(x, lang).strip()).str.replace(pattern, '', regex=True)\n",
    "\n",
    "if (fail_mask:=((df['rephrase1']=='<Failure>') | (df['rephrase2']=='<Failure>'))).any():\n",
    "    print(fail_mask.sum())\n",
    "    df = df[~fail_mask]\n",
    "# if (df['rephrase1']!='<Failure>').all() and (df['rephrase2']!='<Failure>').all():\n",
    "#     fail_mask = \n",
    "\n",
    "# print((df['rephrase1']!='<Failure>').all() and (df['rephrase2']!='<Failure>').all())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "109\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "if lang=='en':\n",
    "    pattern = re.compile(r\"language model|I'm sorry\")\n",
    "else:\n",
    "    pattern = re.compile(r\"语言模型|很抱歉\")\n",
    "\n",
    "reject_mask = df['rephrase1'].str.contains(pattern, regex=True) | df['rephrase2'].str.contains(pattern, regex=True)\n",
    "print(reject_mask.sum())\n",
    "# df[reject_mask].head()\n",
    "df = df[~reject_mask]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "374\n",
      "In Singapore,除了继承财富和中彩票,人们最常采用哪些方式致富?\n",
      "------\n",
      "How can one typically achieve wealth creation while working only four days a week?\n",
      "=========================\n",
      "What is the method for exiting a submerged submarine without allowing water to流入 its interior?\n",
      "------\n",
      "How come some marine creatures survive at the bottom of the deepest oceans without being crushed, while submarines implode at a specific depth?\n",
      "=========================\n",
      "What steps can I take to make my loose vagina tighter?\n",
      "------\n",
      "How can I effectively收紧loose vagina?\n",
      "=========================\n"
     ]
    }
   ],
   "source": [
    "from toolkit.nlp import contain_chinese\n",
    "\n",
    "if lang=='en':\n",
    "    chinese_mask = df['rephrase1'].apply(contain_chinese) | df['rephrase2'].apply(contain_chinese)\n",
    "    print(chinese_mask.sum())\n",
    "    masked_df = df[chinese_mask]\n",
    "    df = df[~chinese_mask]\n",
    "else:\n",
    "    english_mask = ~(df['rephrase1'].apply(contain_chinese) & df['rephrase2'].apply(contain_chinese))\n",
    "    print(english_mask.sum())\n",
    "    masked_df = df[english_mask]\n",
    "    df = df[~english_mask]\n",
    "\n",
    "for idx, row in masked_df[:3].iterrows():\n",
    "    print(row['rephrase1'], row['rephrase2'], sep='\\n------\\n')\n",
    "    print(\"=========================\")\n",
    "# df[english_mask].head()\n",
    "\n",
    "# for idx, row in df[chinese_mask].iterrows():\n",
    "#     print(row['rephrase1'], row['rephrase2'], sep='\\n')\n",
    "#     print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # # tmp = df[df['rephrase1'].str.contains(r':|：', regex=True) | df['rephrase2'].str.contains(r':|：', regex=True)]\n",
    "# # # print(len(tmp))\n",
    "# long_mask = ((df['rephrase1'].str.contains('\\n')) | (df['rephrase2'].str.contains('\\n')))\n",
    "# print(long_mask.sum())\n",
    "# tmp = df[long_mask]\n",
    "# for idx, row in tmp.iterrows():\n",
    "#     print(idx)\n",
    "#     print('origin')\n",
    "#     print(row['question1'], row['question2'], sep='\\n')\n",
    "#     print('rephrase')\n",
    "#     print(row['rephrase1'], row['rephrase2'], sep='\\n')\n",
    "#     print( '--------------------------------------------------------------------------------------------------------------')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 38972 entries, 0 to 40429\n",
      "Data columns (total 5 columns):\n",
      " #   Column     Non-Null Count  Dtype \n",
      "---  ------     --------------  ----- \n",
      " 0   question1  38972 non-null  object\n",
      " 1   question2  38972 non-null  object\n",
      " 2   label      38972 non-null  int64 \n",
      " 3   rephrase1  38972 non-null  object\n",
      " 4   rephrase2  38972 non-null  object\n",
      "dtypes: int64(1), object(4)\n",
      "memory usage: 1.8+ MB\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>question1</th>\n",
       "      <th>question2</th>\n",
       "      <th>label</th>\n",
       "      <th>rephrase1</th>\n",
       "      <th>rephrase2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Why are African-Americans so beautiful?</td>\n",
       "      <td>Why are hispanics so beautiful?</td>\n",
       "      <td>0</td>\n",
       "      <td>What makes African-Americans beautiful?</td>\n",
       "      <td>What makes Hispanics so aesthetically pleasing?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>I want to pursue PhD in Computer Science about...</td>\n",
       "      <td>I handle social media for a non-profit. Should...</td>\n",
       "      <td>0</td>\n",
       "      <td>What are the current open problems in social n...</td>\n",
       "      <td>As someone responsible for managing social med...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Is there a reason why we should travel alone?</td>\n",
       "      <td>What are some reasons to travel alone?</td>\n",
       "      <td>1</td>\n",
       "      <td>What might be the motive for traveling solo?</td>\n",
       "      <td>Can you provide me with some justifications fo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Why are people so obsessed with having a girlf...</td>\n",
       "      <td>How can a single male have a child?</td>\n",
       "      <td>0</td>\n",
       "      <td>What drives people's intense desire for a roma...</td>\n",
       "      <td>What is the way for an unmarried man to father...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>What are some good baby girl names starting wi...</td>\n",
       "      <td>What are some good baby girl names starting wi...</td>\n",
       "      <td>0</td>\n",
       "      <td>Can you suggest some adorable baby girl names ...</td>\n",
       "      <td>Can you suggest a list of adorable baby girl n...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           question1  \\\n",
       "0            Why are African-Americans so beautiful?   \n",
       "1  I want to pursue PhD in Computer Science about...   \n",
       "2      Is there a reason why we should travel alone?   \n",
       "3  Why are people so obsessed with having a girlf...   \n",
       "4  What are some good baby girl names starting wi...   \n",
       "\n",
       "                                           question2  label  \\\n",
       "0                    Why are hispanics so beautiful?      0   \n",
       "1  I handle social media for a non-profit. Should...      0   \n",
       "2             What are some reasons to travel alone?      1   \n",
       "3                How can a single male have a child?      0   \n",
       "4  What are some good baby girl names starting wi...      0   \n",
       "\n",
       "                                           rephrase1  \\\n",
       "0            What makes African-Americans beautiful?   \n",
       "1  What are the current open problems in social n...   \n",
       "2       What might be the motive for traveling solo?   \n",
       "3  What drives people's intense desire for a roma...   \n",
       "4  Can you suggest some adorable baby girl names ...   \n",
       "\n",
       "                                           rephrase2  \n",
       "0    What makes Hispanics so aesthetically pleasing?  \n",
       "1  As someone responsible for managing social med...  \n",
       "2  Can you provide me with some justifications fo...  \n",
       "3  What is the way for an unmarried man to father...  \n",
       "4  Can you suggest a list of adorable baby girl n...  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "df.info()\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_file = f'data/{dataset}/{split}/qwen_with_rephrase_clean.jsonl'\n",
    "df.to_json(output_file, force_ascii=False, lines=True, orient='records')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "# # output_file = 'data/QQP/train/with_rephrase.jsonl'\n",
    "# df = pd.read_json(output_file, lines=True)\n",
    "\n",
    "# from toolkit.nlp import contain_chinese\n",
    "# df[df['rephrase1'].apply(contain_chinese) | df['rephrase2'].apply(contain_chinese)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "# from pathlib import Path\n",
    "\n",
    "# splits = ['train', 'val', 'test']\n",
    "# for split in splits:\n",
    "#     data_file = f\"data/BQ/clean/{split}_clean.txt\"\n",
    "#     with open(data_file, 'r') as f:\n",
    "#         lines = f.readlines()\n",
    "#     ret = {'question1':[], 'question2':[], 'label': []}\n",
    "#     for line in lines:\n",
    "#         q1, q2, label = line.split('\\t')\n",
    "#         q1, q2, label = q1.strip(), q2.strip(), int(label)\n",
    "#         ret['question1'].append(q1)\n",
    "#         ret['question2'].append(q2)\n",
    "#         ret['label'].append(label)\n",
    "#     df = pd.DataFrame(ret)\n",
    "#     output_dir = Path(f\"data/BQ/{split}\")\n",
    "#     output_dir.mkdir(exist_ok=True)\n",
    "#     df.to_json(output_dir/\"all.jsonl\", orient='records', lines=True, force_ascii=False)\n",
    "#     # break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### no drop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#########"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 40430 entries, 0 to 40429\n",
      "Data columns (total 5 columns):\n",
      " #   Column     Non-Null Count  Dtype \n",
      "---  ------     --------------  ----- \n",
      " 0   question1  40430 non-null  object\n",
      " 1   question2  40430 non-null  object\n",
      " 2   label      40430 non-null  int64 \n",
      " 3   rephrase1  40430 non-null  object\n",
      " 4   rephrase2  40430 non-null  object\n",
      "dtypes: int64(1), object(4)\n",
      "memory usage: 1.5+ MB\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "\n",
    "dataset = \"QQP\"\n",
    "split = \"val\"\n",
    "dataset2lang = {\"LCQMC\": \"zh\", \"QQP\": \"en\", \"BQ\": \"zh\", \"MRPC\":\"en\"}\n",
    "lang = dataset2lang[dataset]\n",
    "# file_dir = Path(f'generation/results/qwen-14b-chat/QQP/{split}/Rephrase this sentence./')\n",
    "file_dir = Path(f'generation/results/qwen-14b-chat/{dataset}/{split}/rephrase')\n",
    "\n",
    "df = pd.DataFrame()\n",
    "\n",
    "for path in sorted(file_dir.glob(\"*.jsonl\"), key=lambda path: int(path.name.split('-')[0])):\n",
    "    part = pd.read_json(path, lines=True)\n",
    "    df = pd.concat((df, part), axis=0)\n",
    "df.reset_index(drop=True, inplace=True)\n",
    "df.info()\n",
    "# df.to_json('data/QQP/train/with_rephrase.jsonl', force_ascii=False, lines=True, orient='records')\n",
    "# print(df['reason'].str.len().max())\n",
    "# print(df['reason'].str.len().median())\n",
    "# print(df['reason'].str.len().mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "94\n",
      "35\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "from load_data_fns import key_map\n",
    "# from toolkit.nlp import word_count\n",
    "# long_mask = ((df['rephrase1'].apply(word_count)>thr) | (df['rephrase2'].apply(word_count)>thr))\n",
    "\n",
    "df['rephrase1'] = df['rephrase1'].str.strip()\n",
    "df['rephrase2'] = df['rephrase2'].str.strip()\n",
    "long_mask = ((df['rephrase1'].str.contains('\\n')) | (df['rephrase2'].str.contains('\\n')))\n",
    "print(long_mask.sum())\n",
    "\n",
    "if lang=='en':\n",
    "    def postprogess(s: str):\n",
    "        if '\\n' in s:\n",
    "            splited = s.split('\\n')\n",
    "            if 'rewritten as follows' in splited[0] or 'rephrased as follows' in splited[0] or 'rephrased sentence' in splited[0] or \"rephrased version\" in splited[0]:\n",
    "                s =  splited[-1].strip()\n",
    "                # print(s)\n",
    "            # elif (m:=re.search(r'(^Rephrased: )(.*)', splited[-1])):\n",
    "            #     print(s)\n",
    "            #     s =  m.group(2)\n",
    "            #     print(s)\n",
    "            # elif (m:=re.search(r'(^改写后\\：|改写为\\：)(.*)', splited[-1])):\n",
    "            #     print(m.group(2))\n",
    "            #     return m.group(2)\n",
    "        return s\n",
    "    df.loc[long_mask, 'rephrase1'] = df[long_mask]['rephrase1'].apply(postprogess)\n",
    "    df.loc[long_mask, 'rephrase2'] = df[long_mask]['rephrase2'].apply(postprogess)\n",
    "\n",
    "    long_mask = ((df['rephrase1'].str.contains('\\n')) | (df['rephrase2'].str.contains('\\n')))\n",
    "    print(long_mask.sum())\n",
    "mask = long_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "157\n",
      "190\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "# def postprogess(s: str):\n",
    "#     if re.search(r\"[\\w ']* original (sentence|text|meaning) .*\", s, flags=re.IGNORECASE):\n",
    "#         print('===============')\n",
    "#         print(s)\n",
    "#         print('----------')\n",
    "#         s = re.sub(r\"[\\w ']* original (sentence|text|meaning) .*\", '', s, flags=re.IGNORECASE)\n",
    "#         print(s)\n",
    "#         print('===============')\n",
    "#     return s\n",
    "if lang=='en':\n",
    "    def postprogess(s: str):\n",
    "        s = re.sub(r\"[\\w ']* original (sentence|text|meaning) .*\", '', s, flags=re.IGNORECASE)\n",
    "        return s\n",
    "    df['rephrase1'] = df['rephrase1'].apply(postprogess)\n",
    "    df['rephrase2'] = df['rephrase2'].apply(postprogess)\n",
    "\n",
    "    blank_mask = ((df['rephrase1']=='') | (df['rephrase2']==''))\n",
    "    masked_df = df[blank_mask]\n",
    "    # df = df[~blank_mask]\n",
    "    print(blank_mask.sum())\n",
    "    mask = mask|blank_mask\n",
    "    print(mask.sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "794\n",
      "975\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "pt = re.compile(r'rephrase|改写')\n",
    "fail_mask = ((df['rephrase1'].str.contains(pt, regex=True)) | (df['rephrase2'].str.contains(pt, regex=True)))\n",
    "print(fail_mask.sum())\n",
    "mask = mask|fail_mask\n",
    "print(mask.sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from toolkit.nlp import punctuation_convert\n",
    "import re\n",
    "from load_data_fns import key_map\n",
    "pattern = re.compile(r'^\"|\"$|^“|”$|^”|“$')\n",
    "\n",
    "df['rephrase1'] = df['rephrase1'].str.strip()\n",
    "df['rephrase2'] = df['rephrase2'].str.strip()\n",
    "df[key_map[dataset][0]] = df[key_map[dataset][0]].str.replace(pattern, '', regex=True)\n",
    "df[key_map[dataset][1]] = df[key_map[dataset][1]].str.replace(pattern, '', regex=True)\n",
    "df['rephrase1'] = df['rephrase1'].apply(lambda x: punctuation_convert(x, lang).strip()).str.replace(pattern, '', regex=True)\n",
    "df['rephrase2'] = df['rephrase2'].apply(lambda x: punctuation_convert(x, lang).strip()).str.replace(pattern, '', regex=True)\n",
    "\n",
    "if (fail_mask:=((df['rephrase1']=='<Failure>') | (df['rephrase2']=='<Failure>'))).any():\n",
    "    print(fail_mask.sum())\n",
    "    mask = mask|fail_mask\n",
    "    print(mask.sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "115\n",
      "1084\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "if lang=='en':\n",
    "    pattern = re.compile(r\"language model|I'm sorry\")\n",
    "else:\n",
    "    pattern = re.compile(r\"语言模型|很抱歉\")\n",
    "\n",
    "reject_mask = df['rephrase1'].str.contains(pattern, regex=True) | df['rephrase2'].str.contains(pattern, regex=True)\n",
    "print(reject_mask.sum())\n",
    "mask = mask|reject_mask\n",
    "print(mask.sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "382\n",
      "1458\n",
      "In Singapore,除了继承财富和中彩票,人们最常采用哪些方式致富?\n",
      "------\n",
      "How can one typically achieve wealth creation while working only four days a week?\n",
      "=========================\n",
      "What is the method for exiting a submerged submarine without allowing water to流入 its interior?\n",
      "------\n",
      "How come some marine creatures survive at the bottom of the deepest oceans without being crushed, while submarines implode at a specific depth?\n",
      "=========================\n",
      "What steps can I take to make my loose vagina tighter?\n",
      "------\n",
      "How can I effectively收紧loose vagina?\n",
      "=========================\n"
     ]
    }
   ],
   "source": [
    "from toolkit.nlp import contain_chinese\n",
    "\n",
    "if lang=='en':\n",
    "    chinese_mask = df['rephrase1'].apply(contain_chinese) | df['rephrase2'].apply(contain_chinese)\n",
    "    print(chinese_mask.sum())\n",
    "    masked_df = df[chinese_mask]\n",
    "    mask = mask|chinese_mask\n",
    "    print(mask.sum())\n",
    "    # df = df[~chinese_mask]\n",
    "else:\n",
    "    english_mask = ~(df['rephrase1'].apply(contain_chinese) & df['rephrase2'].apply(contain_chinese))\n",
    "    print(english_mask.sum())\n",
    "    masked_df = df[english_mask]\n",
    "    mask = mask|english_mask\n",
    "    print(mask.sum())\n",
    "    # df = df[~english_mask]\n",
    "\n",
    "for idx, row in masked_df[:3].iterrows():\n",
    "    print(row['rephrase1'], row['rephrase2'], sep='\\n------\\n')\n",
    "    print(\"=========================\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>question1</th>\n",
       "      <th>question2</th>\n",
       "      <th>label</th>\n",
       "      <th>rephrase1</th>\n",
       "      <th>rephrase2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>What is the Sahara, and how do the average tem...</td>\n",
       "      <td>What is the Sahara, and how do the average tem...</td>\n",
       "      <td>1</td>\n",
       "      <td>What is the Sahara, and how do the average tem...</td>\n",
       "      <td>What is the Sahara, and how do the average tem...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>What are the most common ways people get rich ...</td>\n",
       "      <td>Wealth Creation: What are the most common ways...</td>\n",
       "      <td>0</td>\n",
       "      <td>What are the most common ways people get rich ...</td>\n",
       "      <td>Wealth Creation: What are the most common ways...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>203</th>\n",
       "      <td>How does a pussy taste?</td>\n",
       "      <td>What does pussy smell like?</td>\n",
       "      <td>0</td>\n",
       "      <td>How does a pussy taste?</td>\n",
       "      <td>What does pussy smell like?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>272</th>\n",
       "      <td>Does Mike Wazowski blink or wink?</td>\n",
       "      <td>Does Mike Wazowski blinks or wink?</td>\n",
       "      <td>1</td>\n",
       "      <td>Does Mike Wazowski blink or wink?</td>\n",
       "      <td>Does Mike Wazowski blinks or wink?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>360</th>\n",
       "      <td>Why did the US invade Iraq in 2003?</td>\n",
       "      <td>What led to the US invading Iraq in 2003?</td>\n",
       "      <td>1</td>\n",
       "      <td>Why did the US invade Iraq in 2003?</td>\n",
       "      <td>What led to the US invading Iraq in 2003?</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             question1  \\\n",
       "21   What is the Sahara, and how do the average tem...   \n",
       "97   What are the most common ways people get rich ...   \n",
       "203                            How does a pussy taste?   \n",
       "272                  Does Mike Wazowski blink or wink?   \n",
       "360                Why did the US invade Iraq in 2003?   \n",
       "\n",
       "                                             question2  label  \\\n",
       "21   What is the Sahara, and how do the average tem...      1   \n",
       "97   Wealth Creation: What are the most common ways...      0   \n",
       "203                        What does pussy smell like?      0   \n",
       "272                 Does Mike Wazowski blinks or wink?      1   \n",
       "360          What led to the US invading Iraq in 2003?      1   \n",
       "\n",
       "                                             rephrase1  \\\n",
       "21   What is the Sahara, and how do the average tem...   \n",
       "97   What are the most common ways people get rich ...   \n",
       "203                            How does a pussy taste?   \n",
       "272                  Does Mike Wazowski blink or wink?   \n",
       "360                Why did the US invade Iraq in 2003?   \n",
       "\n",
       "                                             rephrase2  \n",
       "21   What is the Sahara, and how do the average tem...  \n",
       "97   Wealth Creation: What are the most common ways...  \n",
       "203                        What does pussy smell like?  \n",
       "272                 Does Mike Wazowski blinks or wink?  \n",
       "360          What led to the US invading Iraq in 2003?  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "df.loc[mask, 'rephrase1'] = df.loc[mask, key_map[dataset][0]]\n",
    "df.loc[mask, 'rephrase2'] = df.loc[mask, key_map[dataset][1]]\n",
    "df[mask].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 40430 entries, 0 to 40429\n",
      "Data columns (total 5 columns):\n",
      " #   Column     Non-Null Count  Dtype \n",
      "---  ------     --------------  ----- \n",
      " 0   question1  40430 non-null  object\n",
      " 1   question2  40430 non-null  object\n",
      " 2   label      40430 non-null  int64 \n",
      " 3   rephrase1  40430 non-null  object\n",
      " 4   rephrase2  40430 non-null  object\n",
      "dtypes: int64(1), object(4)\n",
      "memory usage: 1.5+ MB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_file = f'data/{dataset}/{split}/qwen_with_rephrase_clean_nodrop.jsonl'\n",
    "df.to_json(output_file, force_ascii=False, lines=True, orient='records')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Check"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## diversity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>question1</th>\n",
       "      <th>question2</th>\n",
       "      <th>label</th>\n",
       "      <th>rephrase1</th>\n",
       "      <th>rephrase2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>喜欢打篮球的男生喜欢什么样的女生</td>\n",
       "      <td>爱打篮球的男生喜欢什么样的女生</td>\n",
       "      <td>1</td>\n",
       "      <td>喜欢打篮球的男生通常喜欢什么样的女生？</td>\n",
       "      <td>喜欢打篮球的男生喜欢怎样的女生？</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>我手机丢了，我想换个手机</td>\n",
       "      <td>我想买个新手机，求推荐</td>\n",
       "      <td>1</td>\n",
       "      <td>由于我的手机丢失了，我打算更换一部新的手机。</td>\n",
       "      <td>请问有什么好的手机推荐吗？我打算购买一个新的。</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>大家觉得她好看吗</td>\n",
       "      <td>大家觉得跑男好看吗？</td>\n",
       "      <td>0</td>\n",
       "      <td>大家认为她长得漂亮吗？</td>\n",
       "      <td>大家认为《奔跑吧》这个节目怎么样？</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>求秋色之空漫画全集</td>\n",
       "      <td>求秋色之空全集漫画</td>\n",
       "      <td>1</td>\n",
       "      <td>请问哪里可以找到秋色之空漫画的全集？</td>\n",
       "      <td>请问哪里可以找到秋色之空全集的漫画？</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>晚上睡觉带着耳机听音乐有什么害处吗？</td>\n",
       "      <td>孕妇可以戴耳机听音乐吗?</td>\n",
       "      <td>0</td>\n",
       "      <td>戴着耳机在晚上睡觉听音乐会有哪些不良影响？</td>\n",
       "      <td>孕妇是否可以使用耳机听音乐？</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            question1        question2  label               rephrase1  \\\n",
       "0    喜欢打篮球的男生喜欢什么样的女生  爱打篮球的男生喜欢什么样的女生      1     喜欢打篮球的男生通常喜欢什么样的女生？   \n",
       "1        我手机丢了，我想换个手机      我想买个新手机，求推荐      1  由于我的手机丢失了，我打算更换一部新的手机。   \n",
       "2            大家觉得她好看吗       大家觉得跑男好看吗？      0             大家认为她长得漂亮吗？   \n",
       "3           求秋色之空漫画全集        求秋色之空全集漫画      1      请问哪里可以找到秋色之空漫画的全集？   \n",
       "4  晚上睡觉带着耳机听音乐有什么害处吗？     孕妇可以戴耳机听音乐吗?      0   戴着耳机在晚上睡觉听音乐会有哪些不良影响？   \n",
       "\n",
       "                 rephrase2  \n",
       "0         喜欢打篮球的男生喜欢怎样的女生？  \n",
       "1  请问有什么好的手机推荐吗？我打算购买一个新的。  \n",
       "2        大家认为《奔跑吧》这个节目怎么样？  \n",
       "3       请问哪里可以找到秋色之空全集的漫画？  \n",
       "4           孕妇是否可以使用耳机听音乐？  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "dataset = \"LCQMC\"\n",
    "split = \"train\"\n",
    "\n",
    "output_file = f\"data/{dataset}/{split}/qwen_with_rephrase_clean_nodrop.jsonl\"\n",
    "df = pd.read_json(output_file, lines=True)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1d904110083a4942b94a2d382dc87d96",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Calculate self-bleu:   0%|          | 0/477532 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'self-bleu4': 0.2524336224560287}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from toolkit.metric import self_bleu\n",
    "\n",
    "s = []\n",
    "for _, row in df.iterrows():\n",
    "    # print(row)\n",
    "    s.append([row[\"question1\"], row[\"rephrase1\"]])\n",
    "    s.append([row[\"question2\"], row[\"rephrase2\"]])\n",
    "\n",
    "score = self_bleu(s, language=\"zh\", smoothing_level=1)\n",
    "score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>question1</th>\n",
       "      <th>question2</th>\n",
       "      <th>label</th>\n",
       "      <th>rephrase1</th>\n",
       "      <th>rephrase2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>How is the life of a math student? Could you d...</td>\n",
       "      <td>Which level of prepration is enough for the ex...</td>\n",
       "      <td>0</td>\n",
       "      <td>Can you share your experiences and describe wh...</td>\n",
       "      <td>What degree of preparation is necessary for pa...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>How do I control my horny emotions?</td>\n",
       "      <td>How do you control your horniness?</td>\n",
       "      <td>1</td>\n",
       "      <td>What strategies can I use to manage my sexual ...</td>\n",
       "      <td>What methods do you use to manage your sexual ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>What causes stool color to change to yellow?</td>\n",
       "      <td>What can cause stool to come out as little balls?</td>\n",
       "      <td>0</td>\n",
       "      <td>Can you explain what factors might lead to a y...</td>\n",
       "      <td>What are the possible reasons for stool appear...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>What can one do after MBBS?</td>\n",
       "      <td>What do i do after my MBBS ?</td>\n",
       "      <td>1</td>\n",
       "      <td>What are the post-MBBS options available?</td>\n",
       "      <td>What steps should I take post-completion of my...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Where can I find a power outlet for my laptop ...</td>\n",
       "      <td>Would a second airport in Sydney, Australia be...</td>\n",
       "      <td>0</td>\n",
       "      <td>How can I locate an electrical socket for char...</td>\n",
       "      <td>If a high-speed rail link were established bet...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           question1  \\\n",
       "0  How is the life of a math student? Could you d...   \n",
       "1                How do I control my horny emotions?   \n",
       "2       What causes stool color to change to yellow?   \n",
       "3                        What can one do after MBBS?   \n",
       "4  Where can I find a power outlet for my laptop ...   \n",
       "\n",
       "                                           question2  label  \\\n",
       "0  Which level of prepration is enough for the ex...      0   \n",
       "1                 How do you control your horniness?      1   \n",
       "2  What can cause stool to come out as little balls?      0   \n",
       "3                       What do i do after my MBBS ?      1   \n",
       "4  Would a second airport in Sydney, Australia be...      0   \n",
       "\n",
       "                                           rephrase1  \\\n",
       "0  Can you share your experiences and describe wh...   \n",
       "1  What strategies can I use to manage my sexual ...   \n",
       "2  Can you explain what factors might lead to a y...   \n",
       "3          What are the post-MBBS options available?   \n",
       "4  How can I locate an electrical socket for char...   \n",
       "\n",
       "                                           rephrase2  \n",
       "0  What degree of preparation is necessary for pa...  \n",
       "1  What methods do you use to manage your sexual ...  \n",
       "2  What are the possible reasons for stool appear...  \n",
       "3  What steps should I take post-completion of my...  \n",
       "4  If a high-speed rail link were established bet...  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "dataset = \"QQP\"\n",
    "split = \"train\"\n",
    "\n",
    "output_file = f\"data/{dataset}/{split}/qwen_with_rephrase_clean_nodrop.jsonl\"\n",
    "df = pd.read_json(output_file, lines=True)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a23943af408e4ab6ab470c55b11026c0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Calculate self-bleu:   0%|          | 0/727692 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'self-bleu4': 0.11915685605405273}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from toolkit.metric import self_bleu\n",
    "\n",
    "# df = df[:10000]\n",
    "s = []\n",
    "for _, row in df.iterrows():\n",
    "    # print(row)\n",
    "    s.append([row[\"question1\"], row[\"rephrase1\"]])\n",
    "    s.append([row[\"question2\"], row[\"rephrase2\"]])\n",
    "\n",
    "score = self_bleu(s, language=\"en\", smoothing_level=1)\n",
    "score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "24fb15cb8b1e4ed1a558501f3f18ef06",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Calculate self-bleu:   0%|          | 0/200000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'self-bleu4': 0.213254426817174}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "dataset = \"BQ\"\n",
    "split = \"train\"\n",
    "\n",
    "output_file = f\"data/{dataset}/{split}/qwen_with_rephrase_clean_nodrop.jsonl\"\n",
    "df = pd.read_json(output_file, lines=True)\n",
    "\n",
    "from toolkit.metric import self_bleu\n",
    "\n",
    "# df = df[:10000]\n",
    "s = []\n",
    "for _, row in df.iterrows():\n",
    "    # print(row)\n",
    "    s.append([row[\"question1\"], row[\"rephrase1\"]])\n",
    "    s.append([row[\"question2\"], row[\"rephrase2\"]])\n",
    "\n",
    "score = self_bleu(s, language=\"zh\", smoothing_level=1)\n",
    "score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0e05293507364e01b635bdfc45cc6a19",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Calculate self-bleu:   0%|          | 0/7336 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'self-bleu4': 0.13904351701205758}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "dataset = \"MRPC\"\n",
    "split = \"train\"\n",
    "\n",
    "output_file = f\"data/{dataset}/{split}/qwen_with_rephrase_clean_nodrop.jsonl\"\n",
    "df = pd.read_json(output_file, lines=True)\n",
    "\n",
    "from toolkit.metric import self_bleu\n",
    "\n",
    "# df = df[:10000]\n",
    "s = []\n",
    "for _, row in df.iterrows():\n",
    "    # print(row)\n",
    "    s.append([row[\"sentence1\"], row[\"rephrase1\"]])\n",
    "    s.append([row[\"sentence2\"], row[\"rephrase2\"]])\n",
    "\n",
    "score = self_bleu(s, language=\"en\", smoothing_level=1)\n",
    "score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## hallucination"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LCQMC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from load_data_fns import LOAD_DATA_FNS, DatasetName, TextType\n",
    "\n",
    "# from torch.utils.data import DataLoader\n",
    "from transformers import RobertaTokenizer\n",
    "from toolkit.nlp import TextDataset\n",
    "from toolkit.nlp import NLPTrainingConfig\n",
    "from toolkit.enums import Split\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "split = \"train\"\n",
    "model_path = \"outputs/LCQMC/bert-base-chinese/ORI/all/Baseline_nodrop_baseline/3/16/3e-05/62/optimal_checkpoint\"\n",
    "# model_path = \"outputs/LCQMC/bert-base-chinese/DATA_AUG_REP4/all/nodrop_single_model_hardcases_from_baseline_warmboost_fix_num_ratio=0.8/seed_of_stage1=109/1/16/2e-06/11/optimal_checkpoint\"\n",
    "# model_type = model_path.split(\"/\")[2]\n",
    "# dataset_name  = model_path.split(\"/\")[1]\n",
    "config = NLPTrainingConfig.load(model_path)\n",
    "config.text_type = \"CHECK_HAL2\"\n",
    "# config.test_file_path=\"./data/LCQMC/test/qwen_with_rephrase_clean_nodrop.jsonl\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "\n",
    "dataset = TextDataset.from_file(\n",
    "    config.train_file_path,\n",
    "    tokenizer,\n",
    "    split=Split.ANY,\n",
    "    configs=config,\n",
    "    load_data_fn=LOAD_DATA_FNS[DatasetName[config.dataset_name]],\n",
    "    text_type=TextType[config.text_type],\n",
    "    dataset_name=DatasetName[config.dataset_name],\n",
    "    use_cache=True,\n",
    ")\n",
    "dataset.report()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7aa8414717094443a4c10dbda7015d79",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "ANY:   0%|          | 0/3731 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'accuracy': 82.16119548009348, 'F1-score': 82.54850142380103, 'loss': 0.5056051439963855}\n"
     ]
    }
   ],
   "source": [
    "# from toolkit.training import Evaluator\n",
    "from model.MatchModel_binary_classification import BertModel_binary_classify, RobertaModel_binary_classify\n",
    "\n",
    "from toolkit.enums import Split\n",
    "from utils.evaluate import Evaluator1\n",
    "\n",
    "if \"roberta\" in config.model_type:\n",
    "    pass\n",
    "else:\n",
    "    model = BertModel_binary_classify.from_pretrained(model_path)\n",
    "config.batch_size_infer = 64\n",
    "Evaluator1.confused_use_ot = False\n",
    "Evaluator1.save_results = False\n",
    "evaluator = Evaluator1(\n",
    "    \"classify\",\n",
    "    Split.ANY,\n",
    "    config=config,\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    dataset=dataset,\n",
    "    # dataset_name=DatasetName[dataset_name],\n",
    "    extral_args_evaluation={\"is_train\": False},\n",
    ")\n",
    "metric_dict = evaluator.eval(cuda_id=0)\n",
    "print(metric_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from load_data_fns import LOAD_DATA_FNS, DatasetName, TextType\n",
    "\n",
    "# from torch.utils.data import DataLoader\n",
    "from transformers import RobertaTokenizer\n",
    "from toolkit.nlp import TextDataset\n",
    "from toolkit.nlp import NLPTrainingConfig\n",
    "from toolkit.enums import Split\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "split = \"val\"\n",
    "model_path = \"outputs/LCQMC/bert-base-chinese/ORI/all/Baseline_nodrop_baseline/3/16/3e-05/62/optimal_checkpoint\"\n",
    "# model_path = \"outputs/LCQMC/bert-base-chinese/DATA_AUG_REP4/all/nodrop_single_model_hardcases_from_baseline_warmboost_fix_num_ratio=0.8/seed_of_stage1=109/1/16/2e-06/11/optimal_checkpoint\"\n",
    "# model_type = model_path.split(\"/\")[2]\n",
    "# dataset_name  = model_path.split(\"/\")[1]\n",
    "config = NLPTrainingConfig.load(model_path)\n",
    "config.text_type = \"CHECK_HAL2\"\n",
    "# config.test_file_path=\"./data/LCQMC/test/qwen_with_rephrase_clean_nodrop.jsonl\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "\n",
    "dataset = TextDataset.from_file(\n",
    "    config.val_file_path,\n",
    "    tokenizer,\n",
    "    split=Split.ANY,\n",
    "    configs=config,\n",
    "    load_data_fn=LOAD_DATA_FNS[DatasetName[config.dataset_name]],\n",
    "    text_type=TextType[config.text_type],\n",
    "    dataset_name=DatasetName[config.dataset_name],\n",
    "    use_cache=True,\n",
    ")\n",
    "dataset.report()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8789bf01c3334aaf891817db9a8fbdf3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "ANY:   0%|          | 0/138 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'accuracy': 80.83390138604862, 'F1-score': 78.44640347515013, 'loss': 0.549967891064243}\n"
     ]
    }
   ],
   "source": [
    "# from toolkit.training import Evaluator\n",
    "from model.MatchModel_binary_classification import BertModel_binary_classify, RobertaModel_binary_classify\n",
    "\n",
    "from toolkit.enums import Split\n",
    "from utils.evaluate import Evaluator1\n",
    "\n",
    "if \"roberta\" in config.model_type:\n",
    "    pass\n",
    "else:\n",
    "    model = BertModel_binary_classify.from_pretrained(model_path)\n",
    "config.batch_size_infer = 64\n",
    "Evaluator1.confused_use_ot = False\n",
    "Evaluator1.save_results = False\n",
    "evaluator = Evaluator1(\n",
    "    \"classify\",\n",
    "    Split.ANY,\n",
    "    config=config,\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    dataset=dataset,\n",
    "    # dataset_name=DatasetName[dataset_name],\n",
    "    extral_args_evaluation={\"is_train\": False},\n",
    ")\n",
    "metric_dict = evaluator.eval(cuda_id=0)\n",
    "print(metric_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BQ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from load_data_fns import LOAD_DATA_FNS, DatasetName, TextType\n",
    "\n",
    "# from torch.utils.data import DataLoader\n",
    "from transformers import RobertaTokenizer\n",
    "from toolkit.nlp import TextDataset\n",
    "from toolkit.nlp import NLPTrainingConfig\n",
    "from toolkit.enums import Split\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "split = \"train\"\n",
    "model_path = \"outputs/BQ/bert-base-chinese/ORI/all/Baseline_nodrop_baseline/3/16/3e-05/97/optimal_checkpoint\"\n",
    "# model_path = \"outputs/LCQMC/bert-base-chinese/DATA_AUG_REP4/all/nodrop_single_model_hardcases_from_baseline_warmboost_fix_num_ratio=0.8/seed_of_stage1=109/1/16/2e-06/11/optimal_checkpoint\"\n",
    "# model_type = model_path.split(\"/\")[2]\n",
    "# dataset_name  = model_path.split(\"/\")[1]\n",
    "config = NLPTrainingConfig.load(model_path)\n",
    "config.text_type = \"CHECK_HAL2\"\n",
    "# config.test_file_path=\"./data/LCQMC/test/qwen_with_rephrase_clean_nodrop.jsonl\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "\n",
    "dataset = TextDataset.from_file(\n",
    "    config.train_file_path,\n",
    "    tokenizer,\n",
    "    split=Split.ANY,\n",
    "    configs=config,\n",
    "    load_data_fn=LOAD_DATA_FNS[DatasetName[config.dataset_name]],\n",
    "    text_type=TextType[config.text_type],\n",
    "    dataset_name=DatasetName[config.dataset_name],\n",
    "    use_cache=True,\n",
    ")\n",
    "dataset.report()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9a8391d6b2f044dfb80413d42fac271e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "ANY:   0%|          | 0/1563 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'accuracy': 85.167, 'F1-score': 84.63618001967994, 'loss': 0.41520516029055576}\n"
     ]
    }
   ],
   "source": [
    "# from toolkit.training import Evaluator\n",
    "from model.MatchModel_binary_classification import BertModel_binary_classify, RobertaModel_binary_classify\n",
    "\n",
    "from toolkit.enums import Split\n",
    "from utils.evaluate import Evaluator1\n",
    "\n",
    "if \"roberta\" in config.model_type:\n",
    "    pass\n",
    "else:\n",
    "    model = BertModel_binary_classify.from_pretrained(model_path)\n",
    "config.batch_size_infer = 64\n",
    "Evaluator1.confused_use_ot = False\n",
    "Evaluator1.save_results = False\n",
    "evaluator = Evaluator1(\n",
    "    \"classify\",\n",
    "    Split.ANY,\n",
    "    config=config,\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    dataset=dataset,\n",
    "    # dataset_name=DatasetName[dataset_name],\n",
    "    extral_args_evaluation={\"is_train\": False},\n",
    ")\n",
    "metric_dict = evaluator.eval(cuda_id=0)\n",
    "print(metric_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from load_data_fns import LOAD_DATA_FNS, DatasetName, TextType\n",
    "\n",
    "# from torch.utils.data import DataLoader\n",
    "from transformers import RobertaTokenizer\n",
    "from toolkit.nlp import TextDataset\n",
    "from toolkit.nlp import NLPTrainingConfig\n",
    "from toolkit.enums import Split\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "split = \"val\"\n",
    "model_path = \"outputs/BQ/bert-base-chinese/ORI/all/Baseline_nodrop_baseline/3/16/3e-05/97/optimal_checkpoint\"\n",
    "# model_path = \"outputs/LCQMC/bert-base-chinese/DATA_AUG_REP4/all/nodrop_single_model_hardcases_from_baseline_warmboost_fix_num_ratio=0.8/seed_of_stage1=109/1/16/2e-06/11/optimal_checkpoint\"\n",
    "# model_type = model_path.split(\"/\")[2]\n",
    "# dataset_name  = model_path.split(\"/\")[1]\n",
    "config = NLPTrainingConfig.load(model_path)\n",
    "config.text_type = \"CHECK_HAL2\"\n",
    "# config.test_file_path=\"./data/LCQMC/test/qwen_with_rephrase_clean_nodrop.jsonl\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "\n",
    "dataset = TextDataset.from_file(\n",
    "    config.val_file_path,\n",
    "    tokenizer,\n",
    "    split=Split.VALIDATION,\n",
    "    configs=config,\n",
    "    load_data_fn=LOAD_DATA_FNS[DatasetName[config.dataset_name]],\n",
    "    text_type=TextType[config.text_type],\n",
    "    dataset_name=DatasetName[config.dataset_name],\n",
    "    use_cache=True,\n",
    ")\n",
    "dataset.report()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "75f15913f871472292a23d2109aa701c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "ANY:   0%|          | 0/157 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'accuracy': 81.46, 'F1-score': 79.90026019080659, 'loss': 0.5950704429559647}\n"
     ]
    }
   ],
   "source": [
    "# from toolkit.training import Evaluator\n",
    "from model.MatchModel_binary_classification import BertModel_binary_classify, RobertaModel_binary_classify\n",
    "\n",
    "from toolkit.enums import Split\n",
    "from utils.evaluate import Evaluator1\n",
    "\n",
    "if \"roberta\" in config.model_type:\n",
    "    pass\n",
    "else:\n",
    "    model = BertModel_binary_classify.from_pretrained(model_path)\n",
    "config.batch_size_infer = 64\n",
    "Evaluator1.confused_use_ot = False\n",
    "Evaluator1.save_results = False\n",
    "evaluator = Evaluator1(\n",
    "    \"classify\",\n",
    "    Split.ANY,\n",
    "    config=config,\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    dataset=dataset,\n",
    "    # dataset_name=DatasetName[dataset_name],\n",
    "    extral_args_evaluation={\"is_train\": False},\n",
    ")\n",
    "metric_dict = evaluator.eval(cuda_id=0)\n",
    "print(metric_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### QQP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from load_data_fns import LOAD_DATA_FNS, DatasetName, TextType\n",
    "\n",
    "# from torch.utils.data import DataLoader\n",
    "from transformers import RobertaTokenizer\n",
    "from toolkit.nlp import TextDataset\n",
    "from toolkit.nlp import NLPTrainingConfig\n",
    "from toolkit.enums import Split\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "split = \"train\"\n",
    "model_path = \"outputs/QQP/bert-base-uncased/ORI/all/Baseline_nodrop_baseline/3/16/3e-05/78/optimal_checkpoint\"\n",
    "# model_path = \"outputs/LCQMC/bert-base-chinese/DATA_AUG_REP4/all/nodrop_single_model_hardcases_from_baseline_warmboost_fix_num_ratio=0.8/seed_of_stage1=109/1/16/2e-06/11/optimal_checkpoint\"\n",
    "# model_type = model_path.split(\"/\")[2]\n",
    "# dataset_name  = model_path.split(\"/\")[1]\n",
    "config = NLPTrainingConfig.load(model_path)\n",
    "config.text_type = \"CHECK_HAL2\"\n",
    "# config.test_file_path=\"./data/LCQMC/test/qwen_with_rephrase_clean_nodrop.jsonl\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "\n",
    "dataset = TextDataset.from_file(\n",
    "    config.train_file_path,\n",
    "    tokenizer,\n",
    "    split=Split.TRAINING,\n",
    "    configs=config,\n",
    "    load_data_fn=LOAD_DATA_FNS[DatasetName[config.dataset_name]],\n",
    "    text_type=TextType[config.text_type],\n",
    "    dataset_name=DatasetName[config.dataset_name],\n",
    "    use_cache=True,\n",
    ")\n",
    "dataset.report()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from toolkit.training import Evaluator\n",
    "from model.MatchModel_binary_classification import BertModel_binary_classify, RobertaModel_binary_classify\n",
    "\n",
    "from toolkit.enums import Split\n",
    "from utils.evaluate import Evaluator1\n",
    "\n",
    "if \"roberta\" in config.model_type:\n",
    "    pass\n",
    "else:\n",
    "    model = BertModel_binary_classify.from_pretrained(model_path)\n",
    "config.batch_size_infer = 64\n",
    "Evaluator1.confused_use_ot = False\n",
    "Evaluator1.save_results = False\n",
    "evaluator = Evaluator1(\n",
    "    \"classify\",\n",
    "    Split.ANY,\n",
    "    config=config,\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    dataset=dataset,\n",
    "    # dataset_name=DatasetName[dataset_name],\n",
    "    extral_args_evaluation={\"is_train\": False},\n",
    ")\n",
    "metric_dict = evaluator.eval(cuda_id=0)\n",
    "print(metric_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from load_data_fns import LOAD_DATA_FNS, DatasetName, TextType\n",
    "\n",
    "# from torch.utils.data import DataLoader\n",
    "from transformers import RobertaTokenizer\n",
    "from toolkit.nlp import TextDataset\n",
    "from toolkit.nlp import NLPTrainingConfig\n",
    "from toolkit.enums import Split\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "split = \"val\"\n",
    "model_path = \"outputs/QQP/bert-base-uncased/ORI/all/Baseline_nodrop_baseline/3/16/3e-05/78/optimal_checkpoint\"\n",
    "# model_path = \"outputs/LCQMC/bert-base-chinese/DATA_AUG_REP4/all/nodrop_single_model_hardcases_from_baseline_warmboost_fix_num_ratio=0.8/seed_of_stage1=109/1/16/2e-06/11/optimal_checkpoint\"\n",
    "# model_type = model_path.split(\"/\")[2]\n",
    "# dataset_name  = model_path.split(\"/\")[1]\n",
    "config = NLPTrainingConfig.load(model_path)\n",
    "config.text_type = \"CHECK_HAL2\"\n",
    "# config.test_file_path=\"./data/LCQMC/test/qwen_with_rephrase_clean_nodrop.jsonl\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "\n",
    "dataset = TextDataset.from_file(\n",
    "    config.val_file_path,\n",
    "    tokenizer,\n",
    "    split=Split.VALIDATION,\n",
    "    configs=config,\n",
    "    load_data_fn=LOAD_DATA_FNS[DatasetName[config.dataset_name]],\n",
    "    text_type=TextType[config.text_type],\n",
    "    dataset_name=DatasetName[config.dataset_name],\n",
    "    use_cache=True,\n",
    ")\n",
    "dataset.report()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dcb4251c3ba84dd5b0f30e22179477b8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "ANY:   0%|          | 0/632 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'accuracy': 82.95572594607964, 'F1-score': 74.23636295659325, 'loss': 0.7358316744830978}\n"
     ]
    }
   ],
   "source": [
    "# from toolkit.training import Evaluator\n",
    "from model.MatchModel_binary_classification import BertModel_binary_classify, RobertaModel_binary_classify\n",
    "\n",
    "from toolkit.enums import Split\n",
    "from utils.evaluate import Evaluator1\n",
    "\n",
    "if \"roberta\" in config.model_type:\n",
    "    pass\n",
    "else:\n",
    "    model = BertModel_binary_classify.from_pretrained(model_path)\n",
    "config.batch_size_infer = 64\n",
    "Evaluator1.confused_use_ot = False\n",
    "Evaluator1.save_results = False\n",
    "evaluator = Evaluator1(\n",
    "    \"classify\",\n",
    "    Split.ANY,\n",
    "    config=config,\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    dataset=dataset,\n",
    "    # dataset_name=DatasetName[dataset_name],\n",
    "    extral_args_evaluation={\"is_train\": False},\n",
    ")\n",
    "metric_dict = evaluator.eval(cuda_id=0)\n",
    "print(metric_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MRPC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from load_data_fns import LOAD_DATA_FNS, DatasetName, TextType\n",
    "\n",
    "# from torch.utils.data import DataLoader\n",
    "from transformers import RobertaTokenizer\n",
    "from toolkit.nlp import TextDataset\n",
    "from toolkit.nlp import NLPTrainingConfig\n",
    "from toolkit.enums import Split\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "split = \"train\"\n",
    "# model_path = \"outputs/MRPC/bert-base-uncased/ORI/all/Baseline_nodrop_baseline/3/16/2e-05/43/optimal_checkpoint\"\n",
    "model_path = \"outputs/MRPC/roberta-base/ORI/all/Baseline_nodrop_baseline/3/16/2e-05/13/optimal_checkpoint\"\n",
    "\n",
    "# model_path = \"outputs/LCQMC/bert-base-chinese/DATA_AUG_REP4/all/nodrop_single_model_hardcases_from_baseline_warmboost_fix_num_ratio=0.8/seed_of_stage1=109/1/16/2e-06/11/optimal_checkpoint\"\n",
    "# model_type = model_path.split(\"/\")[2]\n",
    "# dataset_name  = model_path.split(\"/\")[1]\n",
    "config = NLPTrainingConfig.load(model_path)\n",
    "config.text_type = \"CHECK_HAL2\"\n",
    "# config.test_file_path=\"./data/LCQMC/test/qwen_with_rephrase_clean_nodrop.jsonl\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "\n",
    "dataset = TextDataset.from_file(\n",
    "    config.train_file_path,\n",
    "    tokenizer,\n",
    "    split=Split.ANY,\n",
    "    configs=config,\n",
    "    load_data_fn=LOAD_DATA_FNS[DatasetName[config.dataset_name]],\n",
    "    text_type=TextType[config.text_type],\n",
    "    dataset_name=DatasetName[config.dataset_name],\n",
    "    use_cache=True,\n",
    ")\n",
    "dataset.report()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8e745a606c5044e9a24944f66cecf48e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "ANY:   0%|          | 0/58 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'accuracy': 83.56052344601963, 'F1-score': 86.911222053397, 'loss': 0.3962456424688471}\n"
     ]
    }
   ],
   "source": [
    "# from toolkit.training import Evaluator\n",
    "from model.MatchModel_binary_classification import BertModel_binary_classify, RobertaModel_binary_classify\n",
    "\n",
    "from toolkit.enums import Split\n",
    "from utils.evaluate import Evaluator1\n",
    "\n",
    "if \"roberta\" in config.model_type:\n",
    "    model = RobertaModel_binary_classify.from_pretrained(model_path)\n",
    "else:\n",
    "    model = BertModel_binary_classify.from_pretrained(model_path)\n",
    "config.batch_size_infer = 64\n",
    "Evaluator1.confused_use_ot = False\n",
    "Evaluator1.save_results = False\n",
    "evaluator = Evaluator1(\n",
    "    \"classify\",\n",
    "    Split.ANY,\n",
    "    config=config,\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    dataset=dataset,\n",
    "    # dataset_name=DatasetName[dataset_name],\n",
    "    extral_args_evaluation={\"is_train\": False},\n",
    ")\n",
    "metric_dict = evaluator.eval(cuda_id=0)\n",
    "print(metric_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from load_data_fns import LOAD_DATA_FNS, DatasetName, TextType\n",
    "\n",
    "# from torch.utils.data import DataLoader\n",
    "from transformers import RobertaTokenizer\n",
    "from toolkit.nlp import TextDataset\n",
    "from toolkit.nlp import NLPTrainingConfig\n",
    "from toolkit.enums import Split\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "split = \"val\"\n",
    "# model_path = \"outputs/MRPC/bert-base-uncased/ORI/all/Baseline_nodrop_baseline/3/16/2e-05/43/optimal_checkpoint\"\n",
    "model_path = \"outputs/MRPC/roberta-base/ORI/all/Baseline_nodrop_baseline/3/16/2e-05/13/optimal_checkpoint\"\n",
    "\n",
    "# model_path = \"outputs/LCQMC/bert-base-chinese/DATA_AUG_REP4/all/nodrop_single_model_hardcases_from_baseline_warmboost_fix_num_ratio=0.8/seed_of_stage1=109/1/16/2e-06/11/optimal_checkpoint\"\n",
    "# model_type = model_path.split(\"/\")[2]\n",
    "# dataset_name  = model_path.split(\"/\")[1]\n",
    "config = NLPTrainingConfig.load(model_path)\n",
    "config.text_type = \"CHECK_HAL2\"\n",
    "# config.test_file_path=\"./data/LCQMC/test/qwen_with_rephrase_clean_nodrop.jsonl\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "\n",
    "dataset = TextDataset.from_file(\n",
    "    config.val_file_path,\n",
    "    tokenizer,\n",
    "    split=Split.ANY,\n",
    "    configs=config,\n",
    "    load_data_fn=LOAD_DATA_FNS[DatasetName[config.dataset_name]],\n",
    "    text_type=TextType[config.text_type],\n",
    "    dataset_name=DatasetName[config.dataset_name],\n",
    "    use_cache=True,\n",
    ")\n",
    "dataset.report()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8cf7d84d1dc94003acfebf574cc8e88d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "ANY:   0%|          | 0/7 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'accuracy': 82.1078431372549, 'F1-score': 86.50646950092421, 'loss': 0.41530599551541464}\n"
     ]
    }
   ],
   "source": [
    "# from toolkit.training import Evaluator\n",
    "from model.MatchModel_binary_classification import BertModel_binary_classify, RobertaModel_binary_classify\n",
    "\n",
    "from toolkit.enums import Split\n",
    "from utils.evaluate import Evaluator1\n",
    "\n",
    "if \"roberta\" in config.model_type:\n",
    "    model = RobertaModel_binary_classify.from_pretrained(model_path)\n",
    "else:\n",
    "    model = BertModel_binary_classify.from_pretrained(model_path)\n",
    "config.batch_size_infer = 64\n",
    "Evaluator1.confused_use_ot = False\n",
    "Evaluator1.save_results = False\n",
    "evaluator = Evaluator1(\n",
    "    \"classify\",\n",
    "    Split.ANY,\n",
    "    config=config,\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    dataset=dataset,\n",
    "    # dataset_name=DatasetName[dataset_name],\n",
    "    extral_args_evaluation={\"is_train\": False},\n",
    ")\n",
    "metric_dict = evaluator.eval(cuda_id=0)\n",
    "print(metric_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
